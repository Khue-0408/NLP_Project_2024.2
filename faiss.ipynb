{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-09T08:06:39.582368Z",
     "iopub.status.busy": "2025-05-09T08:06:39.581643Z",
     "iopub.status.idle": "2025-05-09T08:06:50.044130Z",
     "shell.execute_reply": "2025-05-09T08:06:50.043300Z",
     "shell.execute_reply.started": "2025-05-09T08:06:39.582339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea\n",
      "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.1.8)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.1.31)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->underthesea) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\n",
      "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl (657 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n",
      "Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "#Download dependencies\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "!pip install underthesea\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Config ---\n",
    "TRAIN_PATH = '/kaggle/input/train_tokenized.csv'\n",
    "CORPUS_PATH = '/kaggle/input/corpus_tokenized.csv'\n",
    "MODEL_SAVE_PATH = './finetuned_model'\n",
    "OUTPUT_EMB = 'corpus_embeddings.npy'\n",
    "OUTPUT_ID_MAP = 'corpus_id_mapping.csv'\n",
    "EPOCHS = 15\n",
    "# Reduce batch size to prevent OOM errors\n",
    "BATCH_SIZE = 64\n",
    "# Introduce gradient accumulation to maintain effective batch size\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "NUM_WORKERS = 4  # Reduced to lower memory pressure\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Enable mixed precision training to reduce memory usage\n",
    "USE_AMP = True\n",
    "\n",
    "# --- Load corpus + train ---\n",
    "corpus_df = pd.read_csv(CORPUS_PATH)\n",
    "corpus_df['context_tokenized'] = corpus_df['context_tokenized'].astype(str)\n",
    "cid_to_text = dict(zip(corpus_df['cid'], corpus_df['context_tokenized']))\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "# --- Create training data more efficiently ---\n",
    "train_data = []\n",
    "for _, row in train_df.iterrows():\n",
    "    q = str(row['question'])\n",
    "    for cid_str in row['cid'].split(','):\n",
    "        try:\n",
    "            cid = int(cid_str.strip())\n",
    "            if cid in cid_to_text:\n",
    "                train_data.append(InputExample(texts=[q, cid_to_text[cid]]))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert '{cid_str}' to integer\")\n",
    "\n",
    "# --- Load model with optimized settings ---\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "# Move model to device after configuration\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# --- Create DataLoader with optimized batch size ---\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True  # Faster data transfer to GPU\n",
    ")\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "model.fit(\n",
    "    train_objectives=[(train_loader, loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    use_amp=USE_AMP,  # Use automatic mixed precision\n",
    "    optimizer_params={'lr': 2e-5},  # Lower learning rate for stability\n",
    "    weight_decay=0.01  # Add weight decay to reduce overfitting\n",
    ")\n",
    "\n",
    "# --- Save model for later inference ---\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "\n",
    "# --- Encode full corpus with memory-efficient batching ---\n",
    "# Process corpus in smaller chunks to avoid OOM\n",
    "ENCODE_BATCH_SIZE = 32\n",
    "texts = corpus_df['context_tokenized'].tolist()\n",
    "ids = corpus_df['cid'].tolist()\n",
    "\n",
    "# Encode in chunks to reduce memory usage\n",
    "def encode_in_chunks(texts, chunk_size=ENCODE_BATCH_SIZE):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        chunk = texts[i:i+chunk_size]\n",
    "        chunk_embeddings = model.encode(\n",
    "            chunk,\n",
    "            batch_size=chunk_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        all_embeddings.append(chunk_embeddings)\n",
    "        # Clear CUDA cache after each chunk\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "embeddings = encode_in_chunks(texts)\n",
    "\n",
    "# --- Save outputs ---\n",
    "np.save(OUTPUT_EMB, embeddings)\n",
    "pd.DataFrame({'cid': ids}).to_csv(OUTPUT_ID_MAP, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T08:56:54.415934Z",
     "iopub.status.busy": "2025-05-09T08:56:54.415228Z",
     "iopub.status.idle": "2025-05-09T08:57:00.462996Z",
     "shell.execute_reply": "2025-05-09T08:57:00.462273Z",
     "shell.execute_reply.started": "2025-05-09T08:56:54.415907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading corpus...\n",
      "Loading embeddings and ID mapping...\n",
      "Original query: phó tổng giám đốc ngân hàng chính sách xã hội được xếp lương theo bảng lương như thế nào\n",
      "Tokenized query: phó tổng giám đốc ngân hàng chính sách xã hội được xếp lương theo bảng lương như thế nào\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e692c19396a34358b49c4ce78185871b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most relevant documents:\n",
      "\n",
      "1. CID: 140864\n",
      "   Similarity score: 0.8029\n",
      "   Text: áp_dụng chế_độ tiền_lương và phụ_cấp quy_định tại nghị_định số cp ngày_tháng năm của chính_phủ quy_định tạm_thời chế_độ tiền_lương mới trong các doanh...\n",
      "\n",
      "2. CID: 218047\n",
      "   Similarity score: 0.6393\n",
      "   Text: ghi_chú giám_đốc học_viện chính_trị quốc_gia hồ chí_minh tổng_biên_tập báo nhân_dân tổng_biên_tập tạp_chí cộng_sản_xếp mức lương chức_vụ theo quy_định...\n",
      "\n",
      "3. CID: 159391\n",
      "   Similarity score: 0.6239\n",
      "   Text: điều_hành hoạt_động của ngân_hàng chính_sách_xã_hội là tổng_giám_đốc tổng_giám_đốc là đại_diện pháp_nhân của ngân_hàng chính_sách_xã_hội giúp_việc tổn...\n",
      "\n",
      "4. CID: 140861\n",
      "   Similarity score: 0.5793\n",
      "   Text: phó tổng_giám_đốc là người giúp tổng_giám_đốc_điều_hành một hoặc một_số lĩnh_vực hoạt_động của ngân_hàng chính_sách_xã_hội theo phân_công của tổng_giá...\n",
      "\n",
      "5. CID: 7557\n",
      "   Similarity score: 0.5770\n",
      "   Text: đối_tượng áp_dụng bảng lương cấp hàm_cơ_yếu những người hiện giữ chức_danh lãnh_đạo do bổ_nhiệm trong tổ_chức cơ_yếu từ trưởng ban hoặc đội_trưởng cơ_...\n",
      "\n",
      "6. CID: 89596\n",
      "   Similarity score: 0.5745\n",
      "   Text: tổng_giám_đốc là đại_diện pháp_nhân của ngân_hàng chính_sách_xã_hội là người chịu trách_nhiệm trước hội_đồng_quản_trị trước pháp_luật về việc tổ_chức ...\n",
      "\n",
      "7. CID: 44833\n",
      "   Similarity score: 0.5696\n",
      "   Text: chủ_tịch phó chủ_tịch và thành_viên hội_đồng quản_lý chuyên_trách giám_đốc phó giám_đốc kế_toán_trưởng và trưởng ban kiểm_soát chuyên_trách sau đây gọ...\n",
      "\n",
      "8. CID: 218046\n",
      "   Similarity score: 0.5579\n",
      "   Text: bảng lương và phụ_cấp chức_vụ lãnh_đạo ban_hành kèm theo quyết_định này bảng lương và các bảng phụ_cấp chức_vụ lãnh_đạo như sau bảng bảng lương chức_v...\n",
      "\n",
      "9. CID: 89597\n",
      "   Similarity score: 0.5425\n",
      "   Text: tổng_giám_đốc phó tổng_giám_đốc là những người không thuộc đối_tượng quy_định tại điều_luật các tổ_chức tín_dụng cư_trú tại việt_nam trong thời_gian đ...\n",
      "\n",
      "10. CID: 118421\n",
      "   Similarity score: 0.5404\n",
      "   Text: điều xếp lương đối_với cán_bộ cấp xã cán_bộ cấp xã có trình_độ sơ_cấp hoặc chưa đào_tạo trình_độ chuyên_môn nghiệp_vụ thực_hiện xếp lương chức_vụ theo...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from underthesea import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# File paths based on the directory structure in the image\n",
    "MODEL_PATH = '/kaggle/input/faissdata/finetuned_model'  # Use the final model or any epoch model\n",
    "CORPUS_PATH = '/kaggle/input/datanlpnew/corpus_tokenized.csv'\n",
    "EMBEDDINGS_PATH = '/kaggle/input/faissdata/corpus_embeddings.npy'\n",
    "ID_MAP_PATH = '/kaggle/input/faissdata/corpus_id_mapping.csv'\n",
    "\n",
    "# Load the fine-tuned model\n",
    "print(\"Loading model...\")\n",
    "model = SentenceTransformer(MODEL_PATH)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading corpus...\")\n",
    "corpus_df = pd.read_csv(CORPUS_PATH)\n",
    "corpus_df['context_tokenized'] = corpus_df['context_tokenized'].astype(str)\n",
    "\n",
    "# Load pre-computed corpus embeddings and IDs\n",
    "print(\"Loading embeddings and ID mapping...\")\n",
    "corpus_embeddings = np.load(EMBEDDINGS_PATH)\n",
    "id_df = pd.read_csv(ID_MAP_PATH)\n",
    "corpus_ids = id_df['cid'].tolist()\n",
    "\n",
    "# Function to perform inference\n",
    "def search_similar_documents(query, top_k=10):\n",
    "    # Tokenize the query using underthesea\n",
    "    tokenized_query = word_tokenize(query)\n",
    "    if isinstance(tokenized_query, list):\n",
    "        tokenized_query = ' '.join(tokenized_query)\n",
    "    print(f\"Tokenized query: {tokenized_query}\")\n",
    "    \n",
    "    # Encode the tokenized query\n",
    "    query_embedding = model.encode(tokenized_query, convert_to_numpy=True, device=device)\n",
    "    \n",
    "    # Calculate similarity with all corpus documents\n",
    "    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n",
    "    \n",
    "    # Get indices of top k most similar documents\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Return the top k most similar documents\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        cid = corpus_ids[idx]\n",
    "        # Find the corresponding document in corpus_df\n",
    "        doc_text = corpus_df[corpus_df['cid'] == cid]['context_tokenized'].values[0]\n",
    "        results.append({\n",
    "            'cid': cid,\n",
    "            'similarity': similarities[idx],\n",
    "            'text': doc_text\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example query (you can replace with any query)\n",
    "    query = \"phó tổng giám đốc ngân hàng chính sách xã hội được xếp lương theo bảng lương như thế nào\"\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    # Get top 10 most similar documents\n",
    "    results = search_similar_documents(query, top_k=10)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"\\nTop 10 most relevant documents:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\n{i+1}. CID: {doc['cid']}\")\n",
    "        print(f\"   Similarity score: {doc['similarity']:.4f}\")\n",
    "        # Print first 150 characters of the document\n",
    "        print(f\"   Text: {doc['text'][:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T09:04:58.298684Z",
     "iopub.status.busy": "2025-05-09T09:04:58.298362Z",
     "iopub.status.idle": "2025-05-09T09:27:27.220254Z",
     "shell.execute_reply": "2025-05-09T09:27:27.219376Z",
     "shell.execute_reply.started": "2025-05-09T09:04:58.298662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading test set...\n",
      "Loading corpus...\n",
      "Loading corpus embeddings...\n",
      "Building FAISS index...\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8244311fba2450fa3aba1911a069dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating MRR:   0%|          | 0/23892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FAISS Evaluation Results (Total Queries: 23892) =====\n",
      "MRR: 0.6459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3dcf62abb6488cbd6c639165264719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Top-K Accuracy:   0%|          | 0/23892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Top-K Accuracy =====\n",
      "Accuracy@5: 0.8231\n",
      "Accuracy@10: 0.9100\n",
      "Accuracy@20: 0.9604\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# File paths\n",
    "MODEL_PATH = '/kaggle/input/faissdata/finetuned_model'\n",
    "TEST_PATH = '/kaggle/input/datanlpnew/test_set.csv'\n",
    "CORPUS_PATH = '/kaggle/input/datanlpnew/corpus_tokenized.csv'\n",
    "EMBEDDINGS_PATH = '/kaggle/input/faissdata/corpus_embeddings.npy'\n",
    "ID_MAP_PATH = '/kaggle/input/faissdata/corpus_id_mapping.csv'\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = SentenceTransformer(MODEL_PATH)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Load test set\n",
    "print(\"Loading test set...\")\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Load corpus\n",
    "print(\"Loading corpus...\")\n",
    "corpus_df = pd.read_csv(CORPUS_PATH)\n",
    "corpus_df['context_tokenized'] = corpus_df['context_tokenized'].astype(str)\n",
    "\n",
    "# Load corpus embeddings and IDs\n",
    "print(\"Loading corpus embeddings...\")\n",
    "corpus_embeddings = np.load(EMBEDDINGS_PATH)\n",
    "id_df = pd.read_csv(ID_MAP_PATH)\n",
    "corpus_ids = id_df['cid'].tolist()\n",
    "\n",
    "# Build FAISS index\n",
    "print(\"Building FAISS index...\")\n",
    "dimension = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Use inner product (cosine similarity for normalized vectors)\n",
    "# Normalize vectors for cosine similarity\n",
    "faiss.normalize_L2(corpus_embeddings)\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "# Define evaluation metrics - calculate overall MRR\n",
    "def calculate_mrr(test_df, search_k=100):  # Use a large k to search through more results\n",
    "    mrr_total = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    # Create a single progress bar\n",
    "    pbar = tqdm(total=len(test_df), desc=\"Evaluating MRR\")\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        # Use the already tokenized question\n",
    "        tokenized_query = str(row['question_tokenized'])\n",
    "        \n",
    "        # Get relevant document IDs for this query\n",
    "        try:\n",
    "            relevant_cids = set(int(cid) for cid in str(row['cid']).split(','))\n",
    "        except ValueError:\n",
    "            # Skip if no valid CIDs\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = model.encode(tokenized_query, convert_to_numpy=True, device=device, show_progress_bar=False)\n",
    "        \n",
    "        # Normalize query vector for cosine similarity\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search using FAISS - use a larger k to find relevant docs that might be further down\n",
    "        distances, indices = index.search(query_embedding, search_k)\n",
    "        \n",
    "        # Calculate rank of first relevant document\n",
    "        rank = float('inf')\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc_cid = corpus_ids[idx]\n",
    "            if doc_cid in relevant_cids:\n",
    "                rank = i + 1\n",
    "                break\n",
    "        \n",
    "        # Update MRR if a relevant document was found\n",
    "        if rank < float('inf'):\n",
    "            mrr_total += 1.0 / rank\n",
    "        \n",
    "        total_queries += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate final MRR\n",
    "    mrr = mrr_total / total_queries if total_queries > 0 else 0\n",
    "    \n",
    "    return mrr, total_queries\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "mrr, total_queries = calculate_mrr(test_df)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n===== FAISS Evaluation Results (Total Queries: {total_queries}) =====\")\n",
    "print(f\"MRR: {mrr:.4f}\")\n",
    "\n",
    "# Also calculate top-k accuracy for common k values\n",
    "def calculate_topk_accuracy(test_df, k_values=[5, 10, 20]):\n",
    "    accuracy_scores = {k: 0 for k in k_values}\n",
    "    total_queries = 0\n",
    "    \n",
    "    # Create a single progress bar\n",
    "    pbar = tqdm(total=len(test_df), desc=\"Evaluating Top-K Accuracy\")\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        tokenized_query = str(row['question_tokenized'])\n",
    "        \n",
    "        try:\n",
    "            relevant_cids = set(int(cid) for cid in str(row['cid']).split(','))\n",
    "        except ValueError:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        query_embedding = model.encode(tokenized_query, convert_to_numpy=True, device=device, show_progress_bar=False)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        max_k = max(k_values)\n",
    "        distances, indices = index.search(query_embedding, max_k)\n",
    "        \n",
    "        # Check if any relevant document is in top-k\n",
    "        for k in k_values:\n",
    "            top_k_indices = indices[0][:k]\n",
    "            top_k_cids = [corpus_ids[idx] for idx in top_k_indices]\n",
    "            if any(cid in relevant_cids for cid in top_k_cids):\n",
    "                accuracy_scores[k] += 1\n",
    "        \n",
    "        total_queries += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate final accuracies\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        results[f'Accuracy@{k}'] = accuracy_scores[k] / total_queries if total_queries > 0 else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate top-k accuracy\n",
    "accuracy_metrics = calculate_topk_accuracy(test_df)\n",
    "print(\"\\n===== Top-K Accuracy =====\")\n",
    "for metric, value in accuracy_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7369721,
     "sourceId": 11739476,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7372222,
     "sourceId": 11743831,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
