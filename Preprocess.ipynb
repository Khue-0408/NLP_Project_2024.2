{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load input files\n",
    "corpus_df = pd.read_csv('/kaggle/input/datanlpnew/corpus.csv')  # columns: 'cid', 'text'\n",
    "train_df = pd.read_csv('/kaggle/input/datanlpnew/train.csv')    # 'cid' column contains lists like \"[31682 31677]\"\n",
    "\n",
    "# Convert corpus to dict for fast lookup\n",
    "cid_to_text = dict(zip(corpus_df['cid'], corpus_df['text']))\n",
    "\n",
    "# Parse cid string like \"[31682 31677 12345]\"\n",
    "def parse_cid_list(cid_str):\n",
    "    if isinstance(cid_str, str):\n",
    "        return [int(x) for x in cid_str.strip(\"[]\").split()]\n",
    "    return [int(cid_str)]\n",
    "\n",
    "# Collect all unique valid cids\n",
    "all_cids = []\n",
    "for raw_cid in train_df['cid']:\n",
    "    parsed_cids = parse_cid_list(raw_cid)\n",
    "    for cid in parsed_cids:\n",
    "        if cid in cid_to_text:\n",
    "            all_cids.append(cid)\n",
    "\n",
    "# Deduplicate\n",
    "unique_valid_cids = sorted(set(all_cids))\n",
    "\n",
    "# Build final DataFrame\n",
    "train_corpus_df = pd.DataFrame({\n",
    "    'cid': unique_valid_cids,\n",
    "    'context': [cid_to_text[cid] for cid in unique_valid_cids]\n",
    "})\n",
    "\n",
    "# Save to file\n",
    "train_corpus_df.to_csv('train_corpus.csv', index=False)\n",
    "\n",
    "# Print stats\n",
    "print(f\"Total questions in train.csv: {len(train_df)}\")\n",
    "print(f\"Total cid references (including duplicates): {len(all_cids)}\")\n",
    "print(f\"Unique valid cids: {len(unique_valid_cids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Cleans text by removing newlines, unwanted characters, normalizing spaces, and trimming. \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace('\\n', ' ')  # âœ… Remove newlines first\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\xa0\\xad]+', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'â€¦+', '', text)\n",
    "    text = re.sub(r'[@#%^&*]+', '', text)\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = re.sub(r'-{2,}', '', text)\n",
    "    text = re.sub(r'_{4,}', '', text)\n",
    "    text = re.sub(r':\\.*', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_id(cid):\n",
    "    \"\"\" Cleans 'cid' and 'qid' columns: removes brackets and normalizes spaces. \"\"\"\n",
    "    cid = re.sub(r'[\\[\\]]', '', cid)\n",
    "    cid = re.sub(r'\\s+', ' ', cid).strip()\n",
    "    return ','.join(cid.split())\n",
    "\n",
    "def remove_punctuation_numbers(text):\n",
    "    \"\"\" Removes punctuation and numbers while keeping letters. \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r'[^\\p{L}\\s]', '', text)\n",
    "    text = re.sub(r' {2,}', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def filter_words(text):\n",
    "    \"\"\" Removes words shorter than 2 or longer than 7 characters and those with duplicate consecutive characters. \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = text.split()\n",
    "    filtered = []\n",
    "    for word in words:\n",
    "        if 2 <= len(word) <= 7 and not re.search(r'(.)\\1', word):\n",
    "            filtered.append(word)\n",
    "    return ' '.join(filtered)\n",
    "def clean_csv(input_file, output_file):\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "\n",
    "    # Clean text columns if they exist\n",
    "    if 'context' in df.columns:\n",
    "        df['context'] = df['context'].astype(str).apply(clean_text).apply(remove_punctuation_numbers).apply(filter_words)\n",
    "    if 'question' in df.columns:\n",
    "        df['question'] = df['question'].astype(str).apply(clean_text).apply(remove_punctuation_numbers).apply(filter_words)\n",
    "    if 'text' in df.columns:\n",
    "        df['text'] = df['text'].astype(str).apply(clean_text).apply(remove_punctuation_numbers).apply(filter_words)\n",
    "\n",
    "    # Clean ID columns if they exist\n",
    "    if 'cid' in df.columns:\n",
    "        df['cid'] = df['cid'].astype(str).apply(clean_id)\n",
    "    if 'qid' in df.columns:\n",
    "        df['qid'] = df['qid'].astype(str).apply(clean_id)\n",
    "\n",
    "    # Save cleaned file\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… Cleaned file saved: {output_file}\")\n",
    "\n",
    "clean_csv('/kaggle/input/datanlpnew/train_corpus.csv', '/kaggle/working/corpus_cleaned.csv')\n",
    "clean_csv('/kaggle/input/datanlpnew/train.csv', '/kaggle/working/train_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "import pandas as pd\n",
    "from underthesea import word_tokenize, pos_tag\n",
    "\n",
    "# Tokenizer and tagger\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(str(text), format=\"text\")\n",
    "    \n",
    "def process_train(train_input, train_output):\n",
    "    df = pd.read_csv(train_input)\n",
    "    print(\"ðŸ”„ Tokenizing and tagging 'question'...\")\n",
    "    df[\"question_tokenized\"] = df[\"question\"].astype(str).apply(tokenize_text)\n",
    "    print(\"ðŸ”„ Tokenizing and tagging 'context'...\")\n",
    "    df[\"context_tokenized\"] = df[\"context\"].astype(str).apply(tokenize_text)\n",
    "    df.to_csv(train_output, index=False)\n",
    "    print(f\"âœ… Tokenized train saved to {train_output}\")\n",
    "\n",
    "def process_corpus(corpus_input, corpus_output):\n",
    "    df = pd.read_csv(corpus_input)\n",
    "    print(\"ðŸ”„ Tokenizing and tagging 'context'...\")\n",
    "    df[\"context_tokenized\"] = df[\"context\"].astype(str).apply(tokenize_text)\n",
    "    df.to_csv(corpus_output, index=False)\n",
    "    print(f\"âœ… Tokenized corpus saved to {corpus_output}\")\n",
    "# Run\n",
    "process_train(\"/kaggle/working/train_cleaned.csv\", \"train_tokenized.csv\")\n",
    "process_corpus(\"/kaggle/working/corpus_cleaned.csv\", \"corpus_tokenized.csv\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from underthesea import pos_tag\n",
    "\n",
    "def compute_idf(tokenized_series):\n",
    "    \"\"\"Computes BM25-style IDF scores from tokenized text.\"\"\"\n",
    "    df_counts = defaultdict(int)\n",
    "    total_docs = len(tokenized_series)\n",
    "\n",
    "    for doc in tokenized_series:\n",
    "        unique_words = set(doc.split())\n",
    "        for word in unique_words:\n",
    "            df_counts[word] += 1\n",
    "\n",
    "    idf_scores = {\n",
    "        word: math.log((total_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "        for word, df in df_counts.items()\n",
    "    }\n",
    "    return idf_scores\n",
    "\n",
    "def build_vocab_with_pos(corpus_csv, output_csv):\n",
    "    df = pd.read_csv(corpus_csv)\n",
    "\n",
    "    # Compute IDF from 'context_tokenized'\n",
    "    idf_scores = compute_idf(df[\"context_tokenized\"].astype(str))\n",
    "\n",
    "    # POS tag only vocabulary words (one word at a time)\n",
    "    vocab_data = []\n",
    "    for word, idf in idf_scores.items():\n",
    "        try:\n",
    "            tagged = pos_tag(word)\n",
    "            pos = tagged[0][1] if tagged else \"\"\n",
    "        except:\n",
    "            pos = \"\"\n",
    "        vocab_data.append({\"word\": word, \"idf\": idf, \"pos\": pos})\n",
    "\n",
    "    pd.DataFrame(vocab_data).to_csv(output_csv, index=False)\n",
    "    print(f\"âœ… Vocabulary saved to {output_csv}\")\n",
    "\n",
    "# Run\n",
    "build_vocab_with_pos(\"/kaggle/working/corpus_tokenized.csv\", \"vocabulary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load tokenized train file\n",
    "df = pd.read_csv(\"train_tokenized.csv\")\n",
    "\n",
    "# Split with 80:20 ratio\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Save to CSV\n",
    "train_set.to_csv(\"train_set.csv\", index=False)\n",
    "test_set.to_csv(\"test_set.csv\", index=False)\n",
    "\n",
    "print(\"âœ… train_set.csv and test_set.csv created with 80:20 split\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7053759,
     "sourceId": 11282161,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
