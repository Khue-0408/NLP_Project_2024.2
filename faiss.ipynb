{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11739476,"sourceType":"datasetVersion","datasetId":7369721},{"sourceId":11743831,"sourceType":"datasetVersion","datasetId":7372222}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Download dependencies\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n!pip install underthesea\n!pip install faiss-cpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T08:06:39.581643Z","iopub.execute_input":"2025-05-09T08:06:39.582368Z","iopub.status.idle":"2025-05-09T08:06:50.044130Z","shell.execute_reply.started":"2025-05-09T08:06:39.582339Z","shell.execute_reply":"2025-05-09T08:06:50.043300Z"}},"outputs":[{"name":"stdout","text":"Collecting underthesea\n  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.1.8)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.4.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.1.31)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.15.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->underthesea) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nDownloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport os\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\n# --- Config ---\nTRAIN_PATH = '/kaggle/input/train_tokenized.csv'\nCORPUS_PATH = '/kaggle/input/corpus_tokenized.csv'\nMODEL_SAVE_PATH = './finetuned_model'\nOUTPUT_EMB = 'corpus_embeddings.npy'\nOUTPUT_ID_MAP = 'corpus_id_mapping.csv'\nEPOCHS = 15\n# Reduce batch size to prevent OOM errors\nBATCH_SIZE = 64\n# Introduce gradient accumulation to maintain effective batch size\nGRADIENT_ACCUMULATION = 4\nNUM_WORKERS = 4  # Reduced to lower memory pressure\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n# Enable mixed precision training to reduce memory usage\nUSE_AMP = True\n\n# --- Load corpus + train ---\ncorpus_df = pd.read_csv(CORPUS_PATH)\ncorpus_df['context_tokenized'] = corpus_df['context_tokenized'].astype(str)\ncid_to_text = dict(zip(corpus_df['cid'], corpus_df['context_tokenized']))\ntrain_df = pd.read_csv(TRAIN_PATH)\n\n# --- Create training data more efficiently ---\ntrain_data = []\nfor _, row in train_df.iterrows():\n    q = str(row['question'])\n    for cid_str in row['cid'].split(','):\n        try:\n            cid = int(cid_str.strip())\n            if cid in cid_to_text:\n                train_data.append(InputExample(texts=[q, cid_to_text[cid]]))\n        except ValueError:\n            print(f\"Could not convert '{cid_str}' to integer\")\n\n# --- Load model with optimized settings ---\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n# Move model to device after configuration\nmodel = model.to(DEVICE)\n\n# --- Create DataLoader with optimized batch size ---\ntrain_loader = DataLoader(\n    train_data, \n    shuffle=True, \n    batch_size=BATCH_SIZE, \n    num_workers=NUM_WORKERS,\n    pin_memory=True  # Faster data transfer to GPU\n)\nloss = losses.MultipleNegativesRankingLoss(model)\nmodel.fit(\n    train_objectives=[(train_loader, loss)],\n    epochs=EPOCHS,\n    warmup_steps=100,\n    show_progress_bar=True,\n    use_amp=USE_AMP,  # Use automatic mixed precision\n    optimizer_params={'lr': 2e-5},  # Lower learning rate for stability\n    weight_decay=0.01  # Add weight decay to reduce overfitting\n)\n\n# --- Save model for later inference ---\nmodel.save(MODEL_SAVE_PATH)\n\n# --- Encode full corpus with memory-efficient batching ---\n# Process corpus in smaller chunks to avoid OOM\nENCODE_BATCH_SIZE = 32\ntexts = corpus_df['context_tokenized'].tolist()\nids = corpus_df['cid'].tolist()\n\n# Encode in chunks to reduce memory usage\ndef encode_in_chunks(texts, chunk_size=ENCODE_BATCH_SIZE):\n    all_embeddings = []\n    for i in range(0, len(texts), chunk_size):\n        chunk = texts[i:i+chunk_size]\n        chunk_embeddings = model.encode(\n            chunk,\n            batch_size=chunk_size,\n            convert_to_numpy=True,\n            show_progress_bar=True,\n            device=DEVICE\n        )\n        all_embeddings.append(chunk_embeddings)\n        # Clear CUDA cache after each chunk\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    return np.vstack(all_embeddings)\n\nembeddings = encode_in_chunks(texts)\n\n# --- Save outputs ---\nnp.save(OUTPUT_EMB, embeddings)\npd.DataFrame({'cid': ids}).to_csv(OUTPUT_ID_MAP, index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom underthesea import word_tokenize\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# File paths based on the directory structure in the image\nMODEL_PATH = '/kaggle/input/faissdata/finetuned_model'  # Use the final model or any epoch model\nCORPUS_PATH = '/kaggle/input/datanlpnew/corpus_tokenized.csv'\nEMBEDDINGS_PATH = '/kaggle/input/faissdata/corpus_embeddings.npy'\nID_MAP_PATH = '/kaggle/input/faissdata/corpus_id_mapping.csv'\n\n# Load the fine-tuned model\nprint(\"Loading model...\")\nmodel = SentenceTransformer(MODEL_PATH)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n# Load corpus\nprint(\"Loading corpus...\")\ncorpus_df = pd.read_csv(CORPUS_PATH)\ncorpus_df['context_tokenized'] = corpus_df['context_tokenized'].astype(str)\n\n# Load pre-computed corpus embeddings and IDs\nprint(\"Loading embeddings and ID mapping...\")\ncorpus_embeddings = np.load(EMBEDDINGS_PATH)\nid_df = pd.read_csv(ID_MAP_PATH)\ncorpus_ids = id_df['cid'].tolist()\n\n# Function to perform inference\ndef search_similar_documents(query, top_k=10):\n    # Tokenize the query using underthesea\n    tokenized_query = word_tokenize(query)\n    if isinstance(tokenized_query, list):\n        tokenized_query = ' '.join(tokenized_query)\n    print(f\"Tokenized query: {tokenized_query}\")\n    \n    # Encode the tokenized query\n    query_embedding = model.encode(tokenized_query, convert_to_numpy=True, device=device)\n    \n    # Calculate similarity with all corpus documents\n    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n    \n    # Get indices of top k most similar documents\n    top_indices = similarities.argsort()[-top_k:][::-1]\n    \n    # Return the top k most similar documents\n    results = []\n    for idx in top_indices:\n        cid = corpus_ids[idx]\n        # Find the corresponding document in corpus_df\n        doc_text = corpus_df[corpus_df['cid'] == cid]['context_tokenized'].values[0]\n        results.append({\n            'cid': cid,\n            'similarity': similarities[idx],\n            'text': doc_text\n        })\n    \n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example query (you can replace with any query)\n    query = \"phó tổng giám đốc ngân hàng chính sách xã hội được xếp lương theo bảng lương như thế nào\"\n    print(f\"Original query: {query}\")\n    \n    # Get top 10 most similar documents\n    results = search_similar_documents(query, top_k=10)\n    \n    # Print the results\n    print(\"\\nTop 10 most relevant documents:\")\n    for i, doc in enumerate(results):\n        print(f\"\\n{i+1}. CID: {doc['cid']}\")\n        print(f\"   Similarity score: {doc['similarity']:.4f}\")\n        # Print first 150 characters of the document\n        print(f\"   Text: {doc['text'][:150]}...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T08:56:54.415228Z","iopub.execute_input":"2025-05-09T08:56:54.415934Z","iopub.status.idle":"2025-05-09T08:57:00.462996Z","shell.execute_reply.started":"2025-05-09T08:56:54.415907Z","shell.execute_reply":"2025-05-09T08:57:00.462273Z"}},"outputs":[{"name":"stdout","text":"Loading model...\nLoading corpus...\nLoading embeddings and ID mapping...\nOriginal query: phó tổng giám đốc ngân hàng chính sách xã hội được xếp lương theo bảng lương như thế nào\nTokenized query: phó tổng giám đốc ngân hàng chính sách xã hội được xếp lương theo bảng lương như thế nào\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e692c19396a34358b49c4ce78185871b"}},"metadata":{}},{"name":"stdout","text":"\nTop 10 most relevant documents:\n\n1. CID: 140864\n   Similarity score: 0.8029\n   Text: áp_dụng chế_độ tiền_lương và phụ_cấp quy_định tại nghị_định số cp ngày_tháng năm của chính_phủ quy_định tạm_thời chế_độ tiền_lương mới trong các doanh...\n\n2. CID: 218047\n   Similarity score: 0.6393\n   Text: ghi_chú giám_đốc học_viện chính_trị quốc_gia hồ chí_minh tổng_biên_tập báo nhân_dân tổng_biên_tập tạp_chí cộng_sản_xếp mức lương chức_vụ theo quy_định...\n\n3. CID: 159391\n   Similarity score: 0.6239\n   Text: điều_hành hoạt_động của ngân_hàng chính_sách_xã_hội là tổng_giám_đốc tổng_giám_đốc là đại_diện pháp_nhân của ngân_hàng chính_sách_xã_hội giúp_việc tổn...\n\n4. CID: 140861\n   Similarity score: 0.5793\n   Text: phó tổng_giám_đốc là người giúp tổng_giám_đốc_điều_hành một hoặc một_số lĩnh_vực hoạt_động của ngân_hàng chính_sách_xã_hội theo phân_công của tổng_giá...\n\n5. CID: 7557\n   Similarity score: 0.5770\n   Text: đối_tượng áp_dụng bảng lương cấp hàm_cơ_yếu những người hiện giữ chức_danh lãnh_đạo do bổ_nhiệm trong tổ_chức cơ_yếu từ trưởng ban hoặc đội_trưởng cơ_...\n\n6. CID: 89596\n   Similarity score: 0.5745\n   Text: tổng_giám_đốc là đại_diện pháp_nhân của ngân_hàng chính_sách_xã_hội là người chịu trách_nhiệm trước hội_đồng_quản_trị trước pháp_luật về việc tổ_chức ...\n\n7. CID: 44833\n   Similarity score: 0.5696\n   Text: chủ_tịch phó chủ_tịch và thành_viên hội_đồng quản_lý chuyên_trách giám_đốc phó giám_đốc kế_toán_trưởng và trưởng ban kiểm_soát chuyên_trách sau đây gọ...\n\n8. CID: 218046\n   Similarity score: 0.5579\n   Text: bảng lương và phụ_cấp chức_vụ lãnh_đạo ban_hành kèm theo quyết_định này bảng lương và các bảng phụ_cấp chức_vụ lãnh_đạo như sau bảng bảng lương chức_v...\n\n9. CID: 89597\n   Similarity score: 0.5425\n   Text: tổng_giám_đốc phó tổng_giám_đốc là những người không thuộc đối_tượng quy_định tại điều_luật các tổ_chức tín_dụng cư_trú tại việt_nam trong thời_gian đ...\n\n10. CID: 118421\n   Similarity score: 0.5404\n   Text: điều xếp lương đối_với cán_bộ cấp xã cán_bộ cấp xã có trình_độ sơ_cấp hoặc chưa đào_tạo trình_độ chuyên_môn nghiệp_vụ thực_hiện xếp lương chức_vụ theo...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport faiss\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.notebook import tqdm\n\n# File paths\nMODEL_PATH = '/kaggle/input/faissdata/finetuned_model'\nTEST_PATH = '/kaggle/input/datanlpnew/test_set.csv'\nCORPUS_PATH = '/kaggle/input/datanlpnew/corpus_tokenized.csv'\nEMBEDDINGS_PATH = '/kaggle/input/faissdata/corpus_embeddings.npy'\nID_MAP_PATH = '/kaggle/input/faissdata/corpus_id_mapping.csv'\n\n# Load model\nprint(\"Loading model...\")\nmodel = SentenceTransformer(MODEL_PATH)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n# Load test set\nprint(\"Loading test set...\")\ntest_df = pd.read_csv(TEST_PATH)\n\n# Load corpus\nprint(\"Loading corpus...\")\ncorpus_df = pd.read_csv(CORPUS_PATH)\ncorpus_df['context_tokenized'] = corpus_df['context_tokenized'].astype(str)\n\n# Load corpus embeddings and IDs\nprint(\"Loading corpus embeddings...\")\ncorpus_embeddings = np.load(EMBEDDINGS_PATH)\nid_df = pd.read_csv(ID_MAP_PATH)\ncorpus_ids = id_df['cid'].tolist()\n\n# Build FAISS index\nprint(\"Building FAISS index...\")\ndimension = corpus_embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)  # Use inner product (cosine similarity for normalized vectors)\n# Normalize vectors for cosine similarity\nfaiss.normalize_L2(corpus_embeddings)\nindex.add(corpus_embeddings)\n\n# Define evaluation metrics - calculate overall MRR\ndef calculate_mrr(test_df, search_k=100):  # Use a large k to search through more results\n    mrr_total = 0\n    total_queries = 0\n    \n    # Create a single progress bar\n    pbar = tqdm(total=len(test_df), desc=\"Evaluating MRR\")\n    \n    for _, row in test_df.iterrows():\n        # Use the already tokenized question\n        tokenized_query = str(row['question_tokenized'])\n        \n        # Get relevant document IDs for this query\n        try:\n            relevant_cids = set(int(cid) for cid in str(row['cid']).split(','))\n        except ValueError:\n            # Skip if no valid CIDs\n            pbar.update(1)\n            continue\n        \n        # Encode query\n        query_embedding = model.encode(tokenized_query, convert_to_numpy=True, device=device, show_progress_bar=False)\n        \n        # Normalize query vector for cosine similarity\n        query_embedding = query_embedding.reshape(1, -1)\n        faiss.normalize_L2(query_embedding)\n        \n        # Search using FAISS - use a larger k to find relevant docs that might be further down\n        distances, indices = index.search(query_embedding, search_k)\n        \n        # Calculate rank of first relevant document\n        rank = float('inf')\n        for i, idx in enumerate(indices[0]):\n            doc_cid = corpus_ids[idx]\n            if doc_cid in relevant_cids:\n                rank = i + 1\n                break\n        \n        # Update MRR if a relevant document was found\n        if rank < float('inf'):\n            mrr_total += 1.0 / rank\n        \n        total_queries += 1\n        pbar.update(1)\n    \n    pbar.close()\n    \n    # Calculate final MRR\n    mrr = mrr_total / total_queries if total_queries > 0 else 0\n    \n    return mrr, total_queries\n\n# Run evaluation\nprint(\"Starting evaluation...\")\nmrr, total_queries = calculate_mrr(test_df)\n\n# Print results\nprint(f\"\\n===== FAISS Evaluation Results (Total Queries: {total_queries}) =====\")\nprint(f\"MRR: {mrr:.4f}\")\n\n# Also calculate top-k accuracy for common k values\ndef calculate_topk_accuracy(test_df, k_values=[5, 10, 20]):\n    accuracy_scores = {k: 0 for k in k_values}\n    total_queries = 0\n    \n    # Create a single progress bar\n    pbar = tqdm(total=len(test_df), desc=\"Evaluating Top-K Accuracy\")\n    \n    for _, row in test_df.iterrows():\n        tokenized_query = str(row['question_tokenized'])\n        \n        try:\n            relevant_cids = set(int(cid) for cid in str(row['cid']).split(','))\n        except ValueError:\n            pbar.update(1)\n            continue\n        \n        query_embedding = model.encode(tokenized_query, convert_to_numpy=True, device=device, show_progress_bar=False)\n        query_embedding = query_embedding.reshape(1, -1)\n        faiss.normalize_L2(query_embedding)\n        \n        max_k = max(k_values)\n        distances, indices = index.search(query_embedding, max_k)\n        \n        # Check if any relevant document is in top-k\n        for k in k_values:\n            top_k_indices = indices[0][:k]\n            top_k_cids = [corpus_ids[idx] for idx in top_k_indices]\n            if any(cid in relevant_cids for cid in top_k_cids):\n                accuracy_scores[k] += 1\n        \n        total_queries += 1\n        pbar.update(1)\n    \n    pbar.close()\n    \n    # Calculate final accuracies\n    results = {}\n    for k in k_values:\n        results[f'Accuracy@{k}'] = accuracy_scores[k] / total_queries if total_queries > 0 else 0\n    \n    return results\n\n# Calculate top-k accuracy\naccuracy_metrics = calculate_topk_accuracy(test_df)\nprint(\"\\n===== Top-K Accuracy =====\")\nfor metric, value in accuracy_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:04:58.298362Z","iopub.execute_input":"2025-05-09T09:04:58.298684Z","iopub.status.idle":"2025-05-09T09:27:27.220254Z","shell.execute_reply.started":"2025-05-09T09:04:58.298662Z","shell.execute_reply":"2025-05-09T09:27:27.219376Z"}},"outputs":[{"name":"stdout","text":"Loading model...\nLoading test set...\nLoading corpus...\nLoading corpus embeddings...\nBuilding FAISS index...\nStarting evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating MRR:   0%|          | 0/23892 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8244311fba2450fa3aba1911a069dbf"}},"metadata":{}},{"name":"stdout","text":"\n===== FAISS Evaluation Results (Total Queries: 23892) =====\nMRR: 0.6459\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating Top-K Accuracy:   0%|          | 0/23892 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce3dcf62abb6488cbd6c639165264719"}},"metadata":{}},{"name":"stdout","text":"\n===== Top-K Accuracy =====\nAccuracy@5: 0.8231\nAccuracy@10: 0.9100\nAccuracy@20: 0.9604\n","output_type":"stream"}],"execution_count":5}]}